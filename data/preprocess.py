import pickle
from collections import Counter
from itertools import chain
import jieba
import gensim
import numpy as np
from sklearn.preprocessing import MultiLabelBinarizer
import re
np.random.seed(1)


def cut_doc(task, text):
    text = re.sub("\s+", " ", text)
    if task.__contains__('douban'):
        return list(jieba.cut(text))
    else:
        return text.split()


def read_data_from_txt(task, file):
    X_label = []
    X_comment = []
    with open(file, 'r', encoding='utf-8') as input:
        for line in input.readlines():
            jobj = eval(line)
            X_label.append(jobj["tags"])

            X_i_comments = []
            for j, reviewobj in enumerate(jobj["all_reviews"]):
                if j == 40: break
                X_i_comments.append(cut_doc(task, reviewobj["comment"]))
            X_comment.append(X_i_comments)

    assert len(X_label) == len(X_comment), "The first dimension must be same."
    return X_label, X_comment


def pad_sequence(X, max_len=20, max_comments=40):
    pad_comment = [0]*max_len
    X_pad = []

    for doc in X:
        X_doc_pad = []
        for sent in doc:
            if len(sent) >= max_len:
                X_doc_pad.append(sent[:max_len])
            else:
                sent = sent + [0] *(max_len - len(sent))
                X_doc_pad.append(sent)

        while len(X_doc_pad) < max_comments:
            X_doc_pad.append(pad_comment)
        X_pad.append(X_doc_pad[-max_comments:])
    return X_pad


def vocab_to_word2vec(fname, vocab:dict):
    """
    Load word2vec from Mikolov
    """
    model = gensim.models.KeyedVectors.load_word2vec_format(fname, binary=True, unicode_errors='ignore')

    word_embeddings = np.zeros(shape=(len(vocab), model.vector_size), dtype='float32')
    count_missing = 0
    for word, idx in vocab.items():
        if word == "<PAD>":
            word_embeddings[idx] = np.zeros(shape=(model.vector_size,) )
            continue

        if model.__contains__(word):
            word_embeddings[idx] = model[word]
        else:
            count_missing += 1

    print(str(len(vocab) - count_missing)+" words found in word2vec.")
    print(str(count_missing)+" words not found, generated by random.")
    print("Embedding matrix of size "+str(np.array(word_embeddings).shape))
    return word_embeddings


def gen_label_dic(X_label, rootpath):
    counter = Counter(chain(*X_label))
    print(counter.items())
    labels = [x[0] for x in counter.most_common()]
    label_dic = {word:i for i, word in enumerate(labels)}
    print(labels)
    pickle.dump(label_dic, open(rootpath+"label_dic.pkl", 'wb'))
    return label_dic


def get_label_relation(label_dic, X_label):
    C = len(label_dic)

    data_adj_matrix = np.zeros(shape=(C, C))
    relation = []
    for labels in X_label:
        for i in range(len(labels)):
            j = i + 1
            while j < len(labels):
                relation.append(str(labels[i]) + "-" + str(labels[j]))
                relation.append(str(labels[j]) + "-" + str(labels[i]))
                j += 1

    rel_con = Counter(relation)
    for rel in rel_con.most_common():
        h, t = rel[0].strip().split("-")
        data_adj_matrix[int(h), int(t)] = rel[1]

    knowledge_adj_matrix = np.zeros(shape=(C, C))
    with open("./music_style_relation_knowledge.txt", 'r', encoding='utf-8') as input:
        for line in input.readlines():
            h, r, t = line.strip().split(",")
            if h in label_dic and t in label_dic:
                h, t = label_dic[h], label_dic[t]
                r = int(r) + 1
                knowledge_adj_matrix[h, t] = r
                knowledge_adj_matrix[t, h] = r

    return data_adj_matrix, knowledge_adj_matrix


def preprocess(rootpath, w2vmodel):
    X_train_label, X_train_comment = read_data_from_txt(rootpath, rootpath+ "train.txt")
    X_dev_label, X_dev_comment = read_data_from_txt(rootpath, rootpath+ "dev.txt")
    X_test_label, X_test_comment = read_data_from_txt(rootpath, rootpath+ "test.txt")

    label_dic = gen_label_dic(X_train_label, rootpath)
    X_train_label = [[label_dic[style] for style in labels] for labels in X_train_label]
    X_dev_label = [[label_dic[style] for style in labels] for labels in X_dev_label]
    X_test_label = [[label_dic[style] for style in labels] for labels in X_test_label]

    data_adj_matrix, knowledge_adj_matrix = get_label_relation(label_dic, X_train_label)
    pickle.dump([data_adj_matrix, knowledge_adj_matrix], open(rootpath+"adj_file.pkl", 'wb'))

    mlb = MultiLabelBinarizer()
    X_train_label = mlb.fit_transform(X_train_label)
    X_dev_label = mlb.transform(X_dev_label)
    X_test_label = mlb.transform(X_test_label)

    counter = Counter(chain(*chain(*X_train_comment)))
    wordslist = [x[0] for x in counter.most_common() if x[1] >= 5]
    words_dic = {word:i+1 for i, word in enumerate(wordslist)}
    words_dic["<PAD>"] = 0
    word_embeddings = vocab_to_word2vec(rootpath + w2vmodel, vocab=words_dic)

    X_train_comment = [[[words_dic[word] for word in comp if word in words_dic] for comp in comments] for comments in X_train_comment]
    X_train_comment = pad_sequence(X_train_comment)
    X_dev_comment = [[[words_dic[word] for word in comp if word in words_dic] for comp in comments] for comments in X_dev_comment]
    X_dev_comment = pad_sequence(X_dev_comment)
    X_test_comment = [[[words_dic[word] for word in comp if word in words_dic] for comp in comments] for comments in X_test_comment]
    X_test_comment = pad_sequence(X_test_comment)

    pickle.dump([X_train_label.tolist(), X_train_comment, word_embeddings], open(rootpath+"X_train.pkl", 'wb'))
    pickle.dump([X_dev_label.tolist(), X_dev_comment], open(rootpath+"X_valid.pkl", 'wb'))
    pickle.dump([X_test_label.tolist(), X_test_comment], open(rootpath+"X_test.pkl", 'wb'))


if __name__ == '__main__':
    preprocess("douban_music/", w2vmodel="douban_w2v.bin")
    preprocess("amazon_music/", w2vmodel="amazon_music_w2v.bin")


# douban
# dict_items([('ost', 554), ('jazz', 226), ('jpop', 462), ('punk', 285), ('rock', 1502), ('folk', 1103),
# ('indie', 1511), ('postpunk', 120), ('alternative', 505), ('electronic', 698), ('pop', 1352), ('hiphop', 156),
# ('postrock', 185), ('r&b', 590), ('metal', 172), ('newage', 309), ('piano', 343), ('darkwave', 63), ('soul', 219),
# ('classical', 199), ('britpop', 433), ('country', 128)])
# ['indie', 'rock', 'pop', 'folk', 'electronic', 'r&b', 'ost', 'alternative', 'jpop', 'britpop', 'piano',
# 'newage', 'punk', 'jazz', 'soul', 'classical', 'postrock', 'metal', 'hiphop',
# 'country', 'postpunk', 'darkwave']

# amazon
# dict_items([('classical', 1137), ('rock', 4161), ('electronic', 969), ('pop', 1798), ('blues', 690),
# ('folk', 838), ('hiphop', 329), ('newage', 291), ('r&b', 905), ('jazz', 1074), ('alternative', 2061),
# ('country', 664), ('soul', 616), ('indie', 482), ('punk', 589), ('postpunk', 294), ('metal', 980),
# ('piano', 20), ('britpop', 77), ('ost', 312)])
# ['rock', 'alternative', 'pop', 'classical', 'jazz', 'metal', 'electronic', 'r&b', 'folk',
# 'blues', 'country', 'soul', 'punk', 'indie', 'hiphop', 'ost', 'postpunk',
# 'newage', 'britpop', 'piano']


